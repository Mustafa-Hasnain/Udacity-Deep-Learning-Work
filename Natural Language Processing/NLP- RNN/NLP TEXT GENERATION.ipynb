{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba54395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d81f4",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d509d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a Function for tokenizer\n",
    "def tokenizer(sentences, num_word = -1):\n",
    "    # Fit a Tokenizer on the corpus\n",
    "    if num_word > -1:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_word)\n",
    "    else:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    return tokenizer\n",
    "\n",
    "def create_lyrical_corpus(dataset, column):\n",
    "    dataset[column] = dataset[column].str.replace('[{}]'.format(string.punctuation),'')\n",
    "    #making it lowerCase\n",
    "    dataset[column] = dataset[column].str.lower()\n",
    "    #making a one long string\n",
    "    lyrics = dataset[column].str.cat()\n",
    "    corpus = lyrics.split('\\n')\n",
    "    #Removing White Spaces\n",
    "    for w in range(len(corpus)):\n",
    "        corpus[w] = corpus[w].rstrip()\n",
    "    # Remove any empty lines\n",
    "    corpus = [l for l in corpus if l != '']\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "461be0eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Ahe's My Kind Of Girl</td>\n",
       "      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n",
       "      <td>Look at her face, it's a wonderful face  \\nAnd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>/a/abba/andante+andante_20002708.html</td>\n",
       "      <td>Take it easy with me, please  \\nTouch me gentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>As Good As New</td>\n",
       "      <td>/a/abba/as+good+as+new_20003033.html</td>\n",
       "      <td>I'll never know why I had to go  \\nWhy I had t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang</td>\n",
       "      <td>/a/abba/bang_20598415.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ABBA</td>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n",
       "      <td>Making somebody happy is a question of give an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  artist                   song                                        link  \\\n",
       "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
       "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
       "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
       "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
       "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
       "\n",
       "                                                text  \n",
       "0  Look at her face, it's a wonderful face  \\nAnd...  \n",
       "1  Take it easy with me, please  \\nTouch me gentl...  \n",
       "2  I'll never know why I had to go  \\nWhy I had t...  \n",
       "3  Making somebody happy is a question of give an...  \n",
       "4  Making somebody happy is a question of give an...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting the data\n",
    "dataset = pd.read_csv('songdata.csv', dtype=str)[:10]\n",
    "dataset.head()\n",
    "#Here We make a function above to process our data easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255d4fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-8a766163dfca>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset[column] = dataset[column].str.replace('[{}]'.format(string.punctuation),'')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
      "495\n"
     ]
    }
   ],
   "source": [
    "# Create the corpus using the 'text' column containing lyrics\n",
    "corpus = create_lyrical_corpus(dataset, 'text')\n",
    "\n",
    "#Tokenizing the corpus\n",
    "tokenizer = tokenizer(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a4fe5b",
   "metadata": {},
   "source": [
    "# Create Sequences and Labels\n",
    "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with texts_to_sequences, but also including the use of N-Grams; creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b34da3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#tokenizing the sentences to sequences\n",
    "sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_seq = token_list[:i+1]\n",
    "        sequences.append(n_gram_seq)\n",
    "        \n",
    "#padding the sequences\n",
    "maxlen = max([len(seq) for seq in sequences])\n",
    "print(maxlen)\n",
    "padded_sequence = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)\n",
    "padded_seq_array = np.array(padded_sequence)\n",
    "\n",
    "#Splitting the Senetnces b/w input and output word predicted\n",
    "input_seq, labels = padded_seq_array[:,:-1], padded_seq_array[:,-1]\n",
    "\n",
    "#Encoding the Labels \n",
    "encoded_label = tf.keras.utils.to_categorical(labels,num_classes=total_words)\n",
    "print(encoded_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "328bf6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "97\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
      "   4] 287\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Checking out how our data is been stored\n",
    "#The tokenizer has just a single word per index\n",
    "print(tokenizer.word_index['know'])\n",
    "print(tokenizer.word_index['feeling'])\n",
    "\n",
    "#Input Sequences will have multiple indexes\n",
    "print(input_seq[5], labels[5])\n",
    "print(encoded_label[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cba919",
   "metadata": {},
   "source": [
    "# Train a Text Generation Model\n",
    "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
    "\n",
    "From there, we should also consider using more epochs than before, as text generation can take a little longer to converge than sentiment analysis, and we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cddca17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(total_words, 64, input_length = maxlen-1))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20)))\n",
    "model.add(tf.keras.layers.Dense(total_words, activation = 'softmax'))\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(from_logits = True), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39f35946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mustafa Hasnain\\anaconda3\\lib\\site-packages\\keras\\backend.py:4846: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 5s 12ms/step - loss: 6.0059 - accuracy: 0.0368\n",
      "Epoch 2/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.4315 - accuracy: 0.0424\n",
      "Epoch 3/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.3474 - accuracy: 0.0439\n",
      "Epoch 4/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.2890 - accuracy: 0.0469\n",
      "Epoch 5/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.2154 - accuracy: 0.0530\n",
      "Epoch 6/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 5.1356 - accuracy: 0.0530\n",
      "Epoch 7/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 5.0624 - accuracy: 0.0550\n",
      "Epoch 8/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.9838 - accuracy: 0.0610\n",
      "Epoch 9/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.9078 - accuracy: 0.0772\n",
      "Epoch 10/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.8206 - accuracy: 0.0757\n",
      "Epoch 11/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.7253 - accuracy: 0.0827\n",
      "Epoch 12/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.6230 - accuracy: 0.0999\n",
      "Epoch 13/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 4.5188 - accuracy: 0.1095\n",
      "Epoch 14/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.4169 - accuracy: 0.1251\n",
      "Epoch 15/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.3046 - accuracy: 0.1357\n",
      "Epoch 16/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.2042 - accuracy: 0.1468\n",
      "Epoch 17/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.1044 - accuracy: 0.1650\n",
      "Epoch 18/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 4.0212 - accuracy: 0.1796\n",
      "Epoch 19/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.9229 - accuracy: 0.1937\n",
      "Epoch 20/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.8382 - accuracy: 0.2114\n",
      "Epoch 21/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.7724 - accuracy: 0.2306\n",
      "Epoch 22/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.6731 - accuracy: 0.2487\n",
      "Epoch 23/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 3.5991 - accuracy: 0.2598 0s - loss: 3.5991 - accuracy: 0.25\n",
      "Epoch 24/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.5211 - accuracy: 0.2805\n",
      "Epoch 25/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.4513 - accuracy: 0.2941\n",
      "Epoch 26/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.3774 - accuracy: 0.3143\n",
      "Epoch 27/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.3263 - accuracy: 0.3310\n",
      "Epoch 28/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.2661 - accuracy: 0.3264\n",
      "Epoch 29/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.2030 - accuracy: 0.3431\n",
      "Epoch 30/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 3.1417 - accuracy: 0.3486 0s - loss: 3.1340 \n",
      "Epoch 31/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 3.0811 - accuracy: 0.3592\n",
      "Epoch 32/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.0131 - accuracy: 0.3708\n",
      "Epoch 33/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.9477 - accuracy: 0.3895\n",
      "Epoch 34/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.8981 - accuracy: 0.4046\n",
      "Epoch 35/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 2.8372 - accuracy: 0.4117\n",
      "Epoch 36/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.7918 - accuracy: 0.4279 0s - loss: 2.8043 - accuracy: \n",
      "Epoch 37/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 2.7525 - accuracy: 0.4273\n",
      "Epoch 38/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.6817 - accuracy: 0.4395\n",
      "Epoch 39/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.6220 - accuracy: 0.4556\n",
      "Epoch 40/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5975 - accuracy: 0.4521\n",
      "Epoch 41/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5428 - accuracy: 0.4697\n",
      "Epoch 42/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5747 - accuracy: 0.4637\n",
      "Epoch 43/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.5613 - accuracy: 0.4687\n",
      "Epoch 44/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4678 - accuracy: 0.4813\n",
      "Epoch 45/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.4047 - accuracy: 0.4990\n",
      "Epoch 46/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3556 - accuracy: 0.5116\n",
      "Epoch 47/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.3091 - accuracy: 0.5161\n",
      "Epoch 48/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 2.2633 - accuracy: 0.5348\n",
      "Epoch 49/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.2135 - accuracy: 0.5444\n",
      "Epoch 50/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 2.1740 - accuracy: 0.5494\n",
      "Epoch 51/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.1263 - accuracy: 0.5626\n",
      "Epoch 52/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.0788 - accuracy: 0.5772\n",
      "Epoch 53/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 2.0402 - accuracy: 0.5863\n",
      "Epoch 54/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 2.0237 - accuracy: 0.5863\n",
      "Epoch 55/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.9913 - accuracy: 0.5928\n",
      "Epoch 56/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.9474 - accuracy: 0.5999\n",
      "Epoch 57/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.9167 - accuracy: 0.6004\n",
      "Epoch 58/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 1.9000 - accuracy: 0.6105\n",
      "Epoch 59/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8856 - accuracy: 0.6100\n",
      "Epoch 60/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.8440 - accuracy: 0.6236\n",
      "Epoch 61/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.8057 - accuracy: 0.6276\n",
      "Epoch 62/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.7613 - accuracy: 0.6423\n",
      "Epoch 63/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.7370 - accuracy: 0.6357 0s - los\n",
      "Epoch 64/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.7070 - accuracy: 0.6408\n",
      "Epoch 65/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6875 - accuracy: 0.6483\n",
      "Epoch 66/200\n",
      "62/62 [==============================] - 1s 17ms/step - loss: 1.6591 - accuracy: 0.6524\n",
      "Epoch 67/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.6249 - accuracy: 0.6604\n",
      "Epoch 68/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.5965 - accuracy: 0.6670\n",
      "Epoch 69/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5837 - accuracy: 0.6710\n",
      "Epoch 70/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.5648 - accuracy: 0.6705\n",
      "Epoch 71/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.5201 - accuracy: 0.6831\n",
      "Epoch 72/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.5012 - accuracy: 0.6887\n",
      "Epoch 73/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4924 - accuracy: 0.6907\n",
      "Epoch 74/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.4602 - accuracy: 0.6963\n",
      "Epoch 75/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.4380 - accuracy: 0.7018\n",
      "Epoch 76/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.4095 - accuracy: 0.7064\n",
      "Epoch 77/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.4234 - accuracy: 0.6983\n",
      "Epoch 78/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 1.4597 - accuracy: 0.6927\n",
      "Epoch 79/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.4312 - accuracy: 0.7018\n",
      "Epoch 80/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.3853 - accuracy: 0.7048\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3453 - accuracy: 0.7230\n",
      "Epoch 82/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.3095 - accuracy: 0.7311\n",
      "Epoch 83/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2946 - accuracy: 0.7386\n",
      "Epoch 84/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2638 - accuracy: 0.7447\n",
      "Epoch 85/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2510 - accuracy: 0.7427\n",
      "Epoch 86/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2208 - accuracy: 0.7513\n",
      "Epoch 87/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.2159 - accuracy: 0.7558\n",
      "Epoch 88/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.1957 - accuracy: 0.7563\n",
      "Epoch 89/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.1650 - accuracy: 0.7639\n",
      "Epoch 90/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.1445 - accuracy: 0.7684\n",
      "Epoch 91/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.1196 - accuracy: 0.7750\n",
      "Epoch 92/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.1003 - accuracy: 0.7841\n",
      "Epoch 93/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.0883 - accuracy: 0.7846\n",
      "Epoch 94/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0774 - accuracy: 0.7820\n",
      "Epoch 95/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0657 - accuracy: 0.7866\n",
      "Epoch 96/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0506 - accuracy: 0.7886\n",
      "Epoch 97/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0371 - accuracy: 0.7916\n",
      "Epoch 98/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0204 - accuracy: 0.7977\n",
      "Epoch 99/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0485 - accuracy: 0.7856\n",
      "Epoch 100/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0262 - accuracy: 0.7926\n",
      "Epoch 101/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 1.0351 - accuracy: 0.7851\n",
      "Epoch 102/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 1.0140 - accuracy: 0.7992\n",
      "Epoch 103/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9761 - accuracy: 0.8042\n",
      "Epoch 104/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9433 - accuracy: 0.8153\n",
      "Epoch 105/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.9305 - accuracy: 0.8169\n",
      "Epoch 106/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.0928 - accuracy: 0.7603\n",
      "Epoch 107/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 1.0326 - accuracy: 0.7785\n",
      "Epoch 108/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.9599 - accuracy: 0.8042\n",
      "Epoch 109/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.9352 - accuracy: 0.8108\n",
      "Epoch 110/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.9204 - accuracy: 0.8148\n",
      "Epoch 111/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.8999 - accuracy: 0.8224\n",
      "Epoch 112/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.8928 - accuracy: 0.8244\n",
      "Epoch 113/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8751 - accuracy: 0.8285\n",
      "Epoch 114/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8553 - accuracy: 0.8355\n",
      "Epoch 115/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8485 - accuracy: 0.8360 0s - loss: 0.8411 - ac\n",
      "Epoch 116/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8314 - accuracy: 0.8391 0s - loss: 0.7843 \n",
      "Epoch 117/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.8153 - accuracy: 0.8436\n",
      "Epoch 118/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.8124 - accuracy: 0.8365\n",
      "Epoch 119/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.8093 - accuracy: 0.8401\n",
      "Epoch 120/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.8025 - accuracy: 0.8401\n",
      "Epoch 121/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7950 - accuracy: 0.8426\n",
      "Epoch 122/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.7736 - accuracy: 0.8451\n",
      "Epoch 123/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.7702 - accuracy: 0.8496\n",
      "Epoch 124/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.7579 - accuracy: 0.8466\n",
      "Epoch 125/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.7432 - accuracy: 0.8577\n",
      "Epoch 126/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7319 - accuracy: 0.8577\n",
      "Epoch 127/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.7243 - accuracy: 0.8547\n",
      "Epoch 128/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.7189 - accuracy: 0.8532\n",
      "Epoch 129/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7405 - accuracy: 0.8486\n",
      "Epoch 130/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7172 - accuracy: 0.8542\n",
      "Epoch 131/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7060 - accuracy: 0.8517\n",
      "Epoch 132/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6986 - accuracy: 0.8607\n",
      "Epoch 133/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.7037 - accuracy: 0.8522\n",
      "Epoch 134/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6944 - accuracy: 0.8486\n",
      "Epoch 135/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6776 - accuracy: 0.8607\n",
      "Epoch 136/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6605 - accuracy: 0.8643\n",
      "Epoch 137/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6514 - accuracy: 0.8628\n",
      "Epoch 138/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6380 - accuracy: 0.8683\n",
      "Epoch 139/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6328 - accuracy: 0.8718\n",
      "Epoch 140/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6207 - accuracy: 0.8688\n",
      "Epoch 141/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6416 - accuracy: 0.8668\n",
      "Epoch 142/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6327 - accuracy: 0.8613\n",
      "Epoch 143/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6234 - accuracy: 0.8683\n",
      "Epoch 144/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6128 - accuracy: 0.8658\n",
      "Epoch 145/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.6059 - accuracy: 0.8643\n",
      "Epoch 146/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6088 - accuracy: 0.8623\n",
      "Epoch 147/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.6019 - accuracy: 0.8693 0s - loss: 0\n",
      "Epoch 148/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5803 - accuracy: 0.8713\n",
      "Epoch 149/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5654 - accuracy: 0.8774\n",
      "Epoch 150/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5584 - accuracy: 0.8804\n",
      "Epoch 151/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5556 - accuracy: 0.8759\n",
      "Epoch 152/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5520 - accuracy: 0.8794\n",
      "Epoch 153/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5454 - accuracy: 0.8769\n",
      "Epoch 154/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5573 - accuracy: 0.8749\n",
      "Epoch 155/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5480 - accuracy: 0.8764\n",
      "Epoch 156/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5299 - accuracy: 0.8814\n",
      "Epoch 157/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5213 - accuracy: 0.8804\n",
      "Epoch 158/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5187 - accuracy: 0.8819\n",
      "Epoch 159/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5107 - accuracy: 0.8809\n",
      "Epoch 160/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.5025 - accuracy: 0.8850\n",
      "Epoch 161/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 1s 12ms/step - loss: 0.5054 - accuracy: 0.8789\n",
      "Epoch 162/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4956 - accuracy: 0.8860\n",
      "Epoch 163/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4909 - accuracy: 0.8829\n",
      "Epoch 164/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4973 - accuracy: 0.8840\n",
      "Epoch 165/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4952 - accuracy: 0.8819\n",
      "Epoch 166/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4945 - accuracy: 0.8799\n",
      "Epoch 167/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4887 - accuracy: 0.8829\n",
      "Epoch 168/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4815 - accuracy: 0.8804\n",
      "Epoch 169/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4921 - accuracy: 0.8794\n",
      "Epoch 170/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4763 - accuracy: 0.8855\n",
      "Epoch 171/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4656 - accuracy: 0.8855\n",
      "Epoch 172/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4707 - accuracy: 0.8840\n",
      "Epoch 173/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.4681 - accuracy: 0.8860\n",
      "Epoch 174/200\n",
      "62/62 [==============================] - 1s 15ms/step - loss: 0.4514 - accuracy: 0.8895\n",
      "Epoch 175/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4494 - accuracy: 0.8900\n",
      "Epoch 176/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4393 - accuracy: 0.8925\n",
      "Epoch 177/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4308 - accuracy: 0.8946 0s - loss: 0.4296 - \n",
      "Epoch 178/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4406 - accuracy: 0.8875\n",
      "Epoch 179/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4331 - accuracy: 0.8920\n",
      "Epoch 180/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4328 - accuracy: 0.8930\n",
      "Epoch 181/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4557 - accuracy: 0.8850\n",
      "Epoch 182/200\n",
      "62/62 [==============================] - 1s 14ms/step - loss: 0.4785 - accuracy: 0.8774\n",
      "Epoch 183/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4349 - accuracy: 0.8900\n",
      "Epoch 184/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4308 - accuracy: 0.8915\n",
      "Epoch 185/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4183 - accuracy: 0.8900\n",
      "Epoch 186/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4110 - accuracy: 0.8930\n",
      "Epoch 187/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4078 - accuracy: 0.8971\n",
      "Epoch 188/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.4000 - accuracy: 0.8981\n",
      "Epoch 189/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3945 - accuracy: 0.8930\n",
      "Epoch 190/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4134 - accuracy: 0.8925\n",
      "Epoch 191/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.4031 - accuracy: 0.8956\n",
      "Epoch 192/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.3967 - accuracy: 0.8951\n",
      "Epoch 193/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.3864 - accuracy: 0.8961\n",
      "Epoch 194/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.3798 - accuracy: 0.8961\n",
      "Epoch 195/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.3776 - accuracy: 0.8961\n",
      "Epoch 196/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.3749 - accuracy: 0.8956\n",
      "Epoch 197/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.3724 - accuracy: 0.8961\n",
      "Epoch 198/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3699 - accuracy: 0.8976\n",
      "Epoch 199/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 0.3696 - accuracy: 0.8991\n",
      "Epoch 200/200\n",
      "62/62 [==============================] - 1s 13ms/step - loss: 0.3692 - accuracy: 0.8971\n"
     ]
    }
   ],
   "source": [
    "#Fitting the mode\n",
    "history  = model.fit(input_seq, encoded_label, epochs = 200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4409e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy'])\n",
      "[6.00592565536499, 5.431546211242676, 5.347366809844971, 5.289025783538818, 5.21537446975708, 5.135605335235596, 5.062378883361816, 4.983809471130371, 4.907795429229736, 4.820634365081787, 4.725269794464111, 4.623023509979248, 4.518833160400391, 4.416851043701172, 4.304625034332275, 4.204214096069336, 4.104437351226807, 4.021244525909424, 3.922865867614746, 3.8381872177124023, 3.7723588943481445, 3.67305326461792, 3.599090576171875, 3.5211102962493896, 3.4512553215026855, 3.3774161338806152, 3.3263473510742188, 3.266148328781128, 3.2030069828033447, 3.141676902770996, 3.081087589263916, 3.013144016265869, 2.947737216949463, 2.8980541229248047, 2.8372321128845215, 2.79183030128479, 2.7524752616882324, 2.681703805923462, 2.6219918727874756, 2.597459077835083, 2.5427680015563965, 2.574738025665283, 2.561309814453125, 2.467752456665039, 2.4046614170074463, 2.3555989265441895, 2.3091461658477783, 2.2633132934570312, 2.2135024070739746, 2.1740026473999023, 2.12634015083313, 2.078803777694702, 2.0401523113250732, 2.0236942768096924, 1.991292119026184, 1.947389841079712, 1.9166826009750366, 1.9000365734100342, 1.8855737447738647, 1.844007968902588, 1.8057172298431396, 1.7612611055374146, 1.7370134592056274, 1.7070295810699463, 1.6875176429748535, 1.6591438055038452, 1.6249457597732544, 1.5964573621749878, 1.5837234258651733, 1.5648378133773804, 1.5200682878494263, 1.5011848211288452, 1.4924224615097046, 1.4602233171463013, 1.437950611114502, 1.4094817638397217, 1.4234086275100708, 1.4597097635269165, 1.4311883449554443, 1.3852722644805908, 1.3453303575515747, 1.3095169067382812, 1.294602870941162, 1.2638498544692993, 1.250961184501648, 1.2208170890808105, 1.2158786058425903, 1.195671558380127, 1.1649744510650635, 1.1444751024246216, 1.1196497678756714, 1.1002912521362305, 1.0882571935653687, 1.0773842334747314, 1.065679669380188, 1.0506305694580078, 1.0371381044387817, 1.0204015970230103, 1.0484520196914673, 1.0262004137039185, 1.035139799118042, 1.0139946937561035, 0.9761094450950623, 0.9432624578475952, 0.9304796457290649, 1.092771053314209, 1.0326300859451294, 0.9599480628967285, 0.9352182149887085, 0.9203572869300842, 0.899928092956543, 0.8927797675132751, 0.8750602006912231, 0.8552829027175903, 0.8485020399093628, 0.8313882350921631, 0.8153491616249084, 0.8124355673789978, 0.8092832565307617, 0.8024610877037048, 0.7950350642204285, 0.7736496925354004, 0.7701888680458069, 0.7579275369644165, 0.743205189704895, 0.7318821549415588, 0.7242553234100342, 0.7188597917556763, 0.7405039072036743, 0.7171508073806763, 0.7060200572013855, 0.6985743641853333, 0.7036766409873962, 0.6944152116775513, 0.6776145696640015, 0.6604712009429932, 0.6513778567314148, 0.6380445957183838, 0.6328086853027344, 0.6206838488578796, 0.6415581703186035, 0.6327255368232727, 0.6233723163604736, 0.6127548217773438, 0.6059187054634094, 0.6087835431098938, 0.6018896102905273, 0.5803471803665161, 0.5653606653213501, 0.558400571346283, 0.555599570274353, 0.5520361065864563, 0.5453624725341797, 0.5572705864906311, 0.5480461120605469, 0.5299351811408997, 0.5213202238082886, 0.5186708569526672, 0.510735034942627, 0.5024652481079102, 0.5053678154945374, 0.4955657124519348, 0.49091511964797974, 0.4972532093524933, 0.4952484667301178, 0.49451401829719543, 0.4886901080608368, 0.48148873448371887, 0.4921239912509918, 0.47631293535232544, 0.4655757248401642, 0.4707051217556, 0.46809902787208557, 0.45138898491859436, 0.44935116171836853, 0.4392995238304138, 0.43084830045700073, 0.4406481981277466, 0.4330553412437439, 0.43277254700660706, 0.455738365650177, 0.4785471260547638, 0.4349078834056854, 0.4308422803878784, 0.41828247904777527, 0.4109746217727661, 0.40782099962234497, 0.3999670743942261, 0.39451467990875244, 0.4134213924407959, 0.40312114357948303, 0.3967035114765167, 0.38638627529144287, 0.37980568408966064, 0.377638041973114, 0.3749471604824066, 0.3723844885826111, 0.3699403405189514, 0.36957603693008423, 0.3691509962081909]\n"
     ]
    }
   ],
   "source": [
    "columns = history.history\n",
    "print(columns.keys())\n",
    "print(columns['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9207d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_graph(columns, string):\n",
    "    plt.plot(columns[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()\n",
    "plot_graph(columns, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a9ece",
   "metadata": {},
   "source": [
    "# Generate new lyrics!\n",
    "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e8f4fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love You, make, me, sing, make, me, mad, make, good, and, would, would, would, would, would, would, chiquitita, happy, care, chiquitita, making, andante, do, have, alone, stood, at, sailing, tumbling, down, good, as, new, anywhere, anywhere, would, ride, shoulder, good, care, life, love, do, do, do, only, sing, more, making, me, strong, good, care, new, but, good, care, new, care, care, care, do, been, evening, evening, eye, evening, evening, eye, eye, evening, eye, evening, eye, evening, eye, evening, eye, evening, eye, eye, evening, eye, evening, eye, evening, evening, eye, evening, eye, evening, eye, evening, evening, eye, evening, eye, evening, eye, evening, eye\n"
     ]
    }
   ],
   "source": [
    "new_text = 'I love You'\n",
    "next_words = 100\n",
    "for i in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([new_text])[0]\n",
    "    token_list = tf.keras.preprocessing.sequence.pad_sequences([token_list], maxlen=maxlen-1)\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    out_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            out_word = word\n",
    "            break\n",
    "    new_text = new_text + ', ' + out_word\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9d3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
