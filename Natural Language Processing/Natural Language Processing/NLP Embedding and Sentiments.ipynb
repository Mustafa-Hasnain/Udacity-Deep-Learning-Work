{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d72c779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c746f450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Good case Excellent value.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The mic is great.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>I have to jiggle the plug to get it to line up...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>If you have several dozen or several hundred c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>If you are Razr owner...you must have this!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Needless to say I wasted my money.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>What a waste of money and time!.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  sentiment\n",
       "0           0  So there is no way for me to plug it in here i...          0\n",
       "1           1                         Good case Excellent value.          1\n",
       "2           2                             Great for the jawbone.          1\n",
       "3           3  Tied to charger for conversations lasting more...          0\n",
       "4           4                                  The mic is great.          1\n",
       "5           5  I have to jiggle the plug to get it to line up...          0\n",
       "6           6  If you have several dozen or several hundred c...          0\n",
       "7           7        If you are Razr owner...you must have this!          1\n",
       "8           8                 Needless to say I wasted my money.          0\n",
       "9           9                   What a waste of money and time!.          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('combined_data.csv')\n",
    "\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f71cd55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So there is no way for me to plug it in here in the US unless I go by a converter.', 'Good case Excellent value.', 'Great for the jawbone.', 'Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!', 'The mic is great.', 'I have to jiggle the plug to get it to line up right to get decent volume.', 'If you have several dozen or several hundred contacts then imagine the fun of sending each of them one by one.', 'If you are Razr owner...you must have this!', 'Needless to say I wasted my money.', 'What a waste of money and time!.']\n"
     ]
    }
   ],
   "source": [
    "sentences = dataset['text'].tolist()\n",
    "labels = dataset['sentiment'].tolist()\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05ded737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1593\n",
      "1593\n"
     ]
    }
   ],
   "source": [
    "# Separate out the sentences and labels into training and test sets\n",
    "\n",
    "training_size = int(len(sentences) * 0.8) #To get 80% percent of data frfom training\n",
    "print(training_size)\n",
    "\n",
    "training_sentences = sentences[0:training_size]\n",
    "testing_sentences = sentences[training_size:] #Rest after training set will be testing data\n",
    "\n",
    "labels_size = int(len(labels) * 0.8)\n",
    "print(labels_size)\n",
    "\n",
    "training_labels = labels[0:labels_size]\n",
    "testing_labels = labels[labels_size:]\n",
    "\n",
    "# Make labels into numpy arrays for use with the network later\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5996e",
   "metadata": {},
   "source": [
    "# Tokenize the dataset\n",
    "Tokenize the dataset, including padding and OOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140b3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size,oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "training_padded = pad_sequences(training_sequences,maxlen=max_length, padding=padding_type, \n",
    "                       truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length, padding=padding_type, \n",
    "                       truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b12d6f",
   "metadata": {},
   "source": [
    "# Review a Sequence\n",
    "Let's quickly take a look at one of the padded sequences to ensure everything above worked appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f26a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good case excellent value ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Good case Excellent value.\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(training_padded[1]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e2b44e",
   "metadata": {},
   "source": [
    "# Train a Basic Sentiment Model with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "009dd0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 100, 16)           16000     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 9606      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 25,613\n",
      "Trainable params: 25,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(units = 6, activation = 'relu'))\n",
    "model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1621bc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "50/50 [==============================] - 1s 6ms/step - loss: 0.6940 - accuracy: 0.5009 - val_loss: 0.6946 - val_accuracy: 0.4110\n",
      "Epoch 2/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6928 - accuracy: 0.5229 - val_loss: 0.6954 - val_accuracy: 0.4110\n",
      "Epoch 3/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6926 - accuracy: 0.5229 - val_loss: 0.6962 - val_accuracy: 0.4110\n",
      "Epoch 4/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6925 - accuracy: 0.5229 - val_loss: 0.6970 - val_accuracy: 0.4110\n",
      "Epoch 5/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5229 - val_loss: 0.6978 - val_accuracy: 0.4110\n",
      "Epoch 6/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.5229 - val_loss: 0.6982 - val_accuracy: 0.4110\n",
      "Epoch 7/15\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6921 - accuracy: 0.5229 - val_loss: 0.6988 - val_accuracy: 0.4110\n",
      "Epoch 8/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6920 - accuracy: 0.5229 - val_loss: 0.6993 - val_accuracy: 0.4110\n",
      "Epoch 9/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6921 - accuracy: 0.5210 - val_loss: 0.6955 - val_accuracy: 0.4436\n",
      "Epoch 10/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.6742 - accuracy: 0.6466 - val_loss: 0.6417 - val_accuracy: 0.6892\n",
      "Epoch 11/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.5607 - accuracy: 0.7903 - val_loss: 0.5617 - val_accuracy: 0.7669\n",
      "Epoch 12/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.4429 - accuracy: 0.8945 - val_loss: 0.5461 - val_accuracy: 0.7945\n",
      "Epoch 13/15\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3755 - accuracy: 0.9466 - val_loss: 0.5402 - val_accuracy: 0.7769\n",
      "Epoch 14/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3295 - accuracy: 0.9755 - val_loss: 0.5542 - val_accuracy: 0.7744\n",
      "Epoch 15/15\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.9818 - val_loss: 0.5523 - val_accuracy: 0.7845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d1c4322b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(training_padded, training_labels_final, epochs=15, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3471a7f0",
   "metadata": {},
   "source": [
    "# Get files for visualizing the network\n",
    "The code below will download two files for visualizing how your network \"sees\" the sentiment related to each word. Head to http://projector.tensorflow.org/ and load these files, then click the \"Sphereize\" checkbox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d453a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 16)\n"
     ]
    }
   ],
   "source": [
    "# First get the weights of the embedding layer\n",
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb8d78e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Write out the embedding vectors and metadata\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "    word = reverse_word_index[word_num]\n",
    "    embeddings = weights[word_num]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c4b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the files\n",
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('vecs.tsv')\n",
    "    files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d97736",
   "metadata": {},
   "source": [
    "# Predicting Sentiment in New Reviews\n",
    "Now that you've trained and visualized your network, take a look below at how we can predict sentiment in new reviews the network has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bac056ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love this phone', 'I hate spaghetti', 'Everything was cold', 'Everything was hot exactly as I wanted', 'Everything was green', 'the host seated us immediately', 'they gave us free chocolate cake', 'not sure about the wilted flowers on the table', 'only works when I stand on tippy toes', 'does not work when I stand on my head']\n",
      "\n",
      "HOT OFF THE PRESS! HERE ARE SOME NEWLY MINTED, ABSOLUTELY GENUINE REVIEWS!\n",
      "\n",
      "I love this phone\n",
      "[0.60881966]\n",
      "\n",
      "\n",
      "I hate spaghetti\n",
      "[0.04142052]\n",
      "\n",
      "\n",
      "Everything was cold\n",
      "[0.19194427]\n",
      "\n",
      "\n",
      "Everything was hot exactly as I wanted\n",
      "[0.53879076]\n",
      "\n",
      "\n",
      "Everything was green\n",
      "[0.40755448]\n",
      "\n",
      "\n",
      "the host seated us immediately\n",
      "[0.60881966]\n",
      "\n",
      "\n",
      "they gave us free chocolate cake\n",
      "[0.60881966]\n",
      "\n",
      "\n",
      "not sure about the wilted flowers on the table\n",
      "[0.01350993]\n",
      "\n",
      "\n",
      "only works when I stand on tippy toes\n",
      "[0.60881966]\n",
      "\n",
      "\n",
      "does not work when I stand on my head\n",
      "[0.00103199]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict a review   \n",
    "fake_reviews = ['I love this phone', 'I hate spaghetti', \n",
    "                'Everything was cold',\n",
    "                'Everything was hot exactly as I wanted', \n",
    "                'Everything was green', \n",
    "                'the host seated us immediately',\n",
    "                'they gave us free chocolate cake', \n",
    "                'not sure about the wilted flowers on the table',\n",
    "                'only works when I stand on tippy toes', \n",
    "                'does not work when I stand on my head']\n",
    "\n",
    "print(fake_reviews) \n",
    "\n",
    "# Create the sequences\n",
    "padding_type='post'\n",
    "sample_sequences = tokenizer.texts_to_sequences(fake_reviews)\n",
    "fakes_padded = pad_sequences(sample_sequences, padding=padding_type, maxlen=max_length)           \n",
    "\n",
    "print('\\nHOT OFF THE PRESS! HERE ARE SOME NEWLY MINTED, ABSOLUTELY GENUINE REVIEWS!\\n')              \n",
    "\n",
    "classes = model.predict(fakes_padded)\n",
    "\n",
    "# The closer the class is to 1, the more positive the review is deemed to be\n",
    "for x in range(len(fake_reviews)):\n",
    "    print(fake_reviews[x])\n",
    "    print(classes[x])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
